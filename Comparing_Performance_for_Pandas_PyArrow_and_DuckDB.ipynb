{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0194f7",
   "metadata": {},
   "source": [
    "## Comparing Performance for Pandas, PyArrow and DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567da833",
   "metadata": {},
   "source": [
    "### Performing Common Data Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057e9dd",
   "metadata": {},
   "source": [
    "I want to compare the difference in runtime and storage when using Pandas vs PyArrow vs DuckDB and CSV vs Parquet files.\n",
    "\n",
    "The data is from Kaggle: https://www.kaggle.com/datasets/geraldooizx/g-coffee-shop-transaction-202307-to-202506\n",
    "\n",
    "It consists of synthetically generated coffee shop transactions spanning July 2023 to June 2025.\n",
    "\n",
    "I'll focus on the `transaction` subdirectory which consists of separate CSV file per month, totalling to **1.11 GB**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ec296d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "from pathlib import Path\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85c87d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11 GB\n"
     ]
    }
   ],
   "source": [
    "# Get total size of subdirectory.\n",
    "def get_dir_size(dir_path):\n",
    "    total_size = 0\n",
    "    for dirpath, _, filenames in os.walk(dir_path):\n",
    "        for file in filenames:\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "            if not os.path.islink(file_path):\n",
    "                total_size += os.path.getsize(file_path)\n",
    "    return convert_size(total_size)\n",
    "\n",
    "def convert_size(size_bytes):\n",
    "    if size_bytes == 0:\n",
    "        return \"0B\"\n",
    "    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "    i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "    p = math.pow(1024, i)\n",
    "    s = round(size_bytes / p, 2)\n",
    "    return \"%s %s\" % (s, size_name[i])\n",
    "\n",
    "dir_size = get_dir_size(\"./data/transactions\")\n",
    "print(dir_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f90048",
   "metadata": {},
   "source": [
    "The following code was used to create the parquet files from the csv files provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7da13fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parquet files with month partitioning:\n",
    "\n",
    "# csv_dir = Path(\"data/transactions\")\n",
    "# output_dir = Path(\"datasets\")\n",
    "# output_dir.mkdir(parents=True, exist_ok=True)\n",
    "# for csv_file in sorted(csv_dir.glob(\"*.csv\")):\n",
    "#     month_str = csv_file.stem.split('_')[-1]\n",
    "#     df = pd.read_csv(csv_file)\n",
    "#     df['month'] = month_str\n",
    "#     df.to_parquet(\n",
    "#         output_dir,\n",
    "#         engine=\"pyarrow\",\n",
    "#         partition_cols=[\"month\"],\n",
    "#         index=False\n",
    "#     )\n",
    "#     print(f\"Processed {csv_file.name} with partition month={month_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc04911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701.39 MB\n"
     ]
    }
   ],
   "source": [
    "dir_size = get_dir_size(\"./datasets\")\n",
    "print(dir_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3271da9",
   "metadata": {},
   "source": [
    "Storing the same transaction data in Parquet files, partitioned by month, the resulting data is of size **701.39 MB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd5f141e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset in 3.3465 seconds\n",
      "                         transaction_id  store_id  payment_method_id  \\\n",
      "0  2ae6d188-76c2-4095-b861-ab97d3cd9312         4                  5   \n",
      "1  7d0a474d-62f4-442a-96b6-a5df2bda8832         7                  1   \n",
      "2  85f86fef-fddb-4eef-9dc3-1444553e6108         1                  5   \n",
      "3  4c41d179-f809-4d5a-a5d7-acb25ae1fe98         5                  2   \n",
      "4  51e44c8e-4812-4a15-a9f9-9a46b62424d6         8                  5   \n",
      "\n",
      "   voucher_id  user_id  original_amount  discount_applied  final_amount  \\\n",
      "0         NaN      NaN             38.0               0.0          38.0   \n",
      "1         NaN      NaN             33.0               0.0          33.0   \n",
      "2         NaN      NaN             27.0               0.0          27.0   \n",
      "3         NaN      NaN             45.5               0.0          45.5   \n",
      "4         NaN      NaN             27.0               0.0          27.0   \n",
      "\n",
      "            created_at   month  \n",
      "0  2023-07-01 07:00:00  202307  \n",
      "1  2023-07-01 07:00:02  202307  \n",
      "2  2023-07-01 07:00:04  202307  \n",
      "3  2023-07-01 07:00:21  202307  \n",
      "4  2023-07-01 07:00:33  202307  \n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "df1 = pd.read_csv(\"data/transactions/transactions_202307.csv\")\n",
    "df2 = pd.read_csv(\"data/transactions/transactions_202308.csv\")\n",
    "df3 = pd.read_csv(\"data/transactions/transactions_202309.csv\")\n",
    "toc = time.perf_counter()\n",
    "print(f\"Loaded dataset in {toc - tic:0.4f} seconds\")\n",
    "df1[\"month\"] = \"202307\"\n",
    "df2[\"month\"] = \"202308\"\n",
    "df3[\"month\"] = \"202309\"\n",
    "df_from_pd = pd.concat([df1, df2, df3])\n",
    "print(df_from_pd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f84b4fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset in 0.4053 seconds\n",
      "pyarrow.Table\n",
      "transaction_id: string\n",
      "store_id: int64\n",
      "payment_method_id: int64\n",
      "voucher_id: double\n",
      "user_id: double\n",
      "original_amount: double\n",
      "discount_applied: double\n",
      "final_amount: double\n",
      "created_at: string\n",
      "month: int32\n",
      "----\n",
      "transaction_id: [[\"2ae6d188-76c2-4095-b861-ab97d3cd9312\"]]\n",
      "store_id: [[4]]\n",
      "payment_method_id: [[5]]\n",
      "voucher_id: [[null]]\n",
      "user_id: [[null]]\n",
      "original_amount: [[38]]\n",
      "discount_applied: [[0]]\n",
      "final_amount: [[38]]\n",
      "created_at: [[\"2023-07-01 07:00:00\"]]\n",
      "month: [[202307]]\n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "dataset = ds.dataset(\"datasets\", format=\"parquet\", partitioning=\"hive\")\n",
    "filter_months = ds.field(\"month\").isin([\"202307\", \"202308\", \"202309\"])\n",
    "table = dataset.to_table(filter=filter_months)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Loaded dataset in {toc - tic:0.4f} seconds\")\n",
    "top_row = table.slice(0, 1)\n",
    "print(top_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416891a",
   "metadata": {},
   "source": [
    "One run sample found that loading in the first three months of data from:\n",
    "- CSV files with Pandas took **3.2922** seconds\n",
    "- Parquet files with PyArrow took **0.4542** seconds\n",
    "\n",
    "**I've found PyArrow to be roughly 7 times quicker loading in data with Parquet files than Pandas is loading in data from CSV.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c69b9",
   "metadata": {},
   "source": [
    "#### Sorting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa355085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by column in 1.6015 seconds\n",
      "                              transaction_id  store_id  payment_method_id  \\\n",
      "540192  d04def13-2c6a-49de-ab00-829d5c9742b6         8                  5   \n",
      "459259  f506afa3-d30c-42d6-944d-3f5f83080215         2                  5   \n",
      "187612  0e569659-fb7b-4c89-8233-9434ca9d4a28         3                  1   \n",
      "461209  ab1e6405-b734-4082-8a43-c9c6e636942b         7                  4   \n",
      "341074  42cb9509-30f2-454f-adef-a29a83da34ab         4                  4   \n",
      "\n",
      "        voucher_id  user_id  original_amount  discount_applied  final_amount  \\\n",
      "540192         NaN      NaN             90.0               0.0          90.0   \n",
      "459259         NaN      NaN             90.0               0.0          90.0   \n",
      "187612         NaN      NaN             90.0               0.0          90.0   \n",
      "461209         NaN      NaN             90.0               0.0          90.0   \n",
      "341074         NaN      NaN             90.0               0.0          90.0   \n",
      "\n",
      "                 created_at   month  \n",
      "540192  2023-09-28 15:28:11  202309  \n",
      "459259  2023-09-24 14:57:42  202309  \n",
      "187612  2023-07-11 10:29:27  202307  \n",
      "461209  2023-08-25 11:06:50  202308  \n",
      "341074  2023-08-19 10:58:35  202308  \n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "sorted_df = df_from_pd.sort_values('final_amount', ascending=False)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Sorted by column in {toc - tic:0.4f} seconds\")\n",
    "print(sorted_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20282ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by column in 1.0852 seconds\n",
      "pyarrow.Table\n",
      "transaction_id: string\n",
      "store_id: int64\n",
      "payment_method_id: int64\n",
      "voucher_id: double\n",
      "user_id: double\n",
      "original_amount: double\n",
      "discount_applied: double\n",
      "final_amount: double\n",
      "created_at: string\n",
      "month: int32\n",
      "----\n",
      "transaction_id: [[\"2346bcfd-520f-43ba-9599-43934670dfc3\"]]\n",
      "store_id: [[2]]\n",
      "payment_method_id: [[5]]\n",
      "voucher_id: [[null]]\n",
      "user_id: [[null]]\n",
      "original_amount: [[90]]\n",
      "discount_applied: [[0]]\n",
      "final_amount: [[90]]\n",
      "created_at: [[\"2023-07-01 13:49:23\"]]\n",
      "month: [[202307]]\n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "sorted_table = table.sort_by([(\"final_amount\", \"descending\")])\n",
    "toc = time.perf_counter()\n",
    "print(f\"Sorted by column in {toc - tic:0.4f} seconds\")\n",
    "top_row = sorted_table.slice(0, 1)\n",
    "print(top_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a33446",
   "metadata": {},
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46291da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered in 0.1806 seconds\n",
      "                         transaction_id  store_id  payment_method_id  \\\n",
      "0  2ae6d188-76c2-4095-b861-ab97d3cd9312         4                  5   \n",
      "1  7d0a474d-62f4-442a-96b6-a5df2bda8832         7                  1   \n",
      "2  85f86fef-fddb-4eef-9dc3-1444553e6108         1                  5   \n",
      "3  4c41d179-f809-4d5a-a5d7-acb25ae1fe98         5                  2   \n",
      "4  51e44c8e-4812-4a15-a9f9-9a46b62424d6         8                  5   \n",
      "\n",
      "   voucher_id  user_id  original_amount  discount_applied  final_amount  \\\n",
      "0         NaN      NaN             38.0               0.0          38.0   \n",
      "1         NaN      NaN             33.0               0.0          33.0   \n",
      "2         NaN      NaN             27.0               0.0          27.0   \n",
      "3         NaN      NaN             45.5               0.0          45.5   \n",
      "4         NaN      NaN             27.0               0.0          27.0   \n",
      "\n",
      "            created_at   month  \n",
      "0  2023-07-01 07:00:00  202307  \n",
      "1  2023-07-01 07:00:02  202307  \n",
      "2  2023-07-01 07:00:04  202307  \n",
      "3  2023-07-01 07:00:21  202307  \n",
      "4  2023-07-01 07:00:33  202307  \n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "month7_no_voucher = df_from_pd[(df_from_pd[\"month\"] == \"202307\") & (df_from_pd[\"discount_applied\"] == 0)]\n",
    "toc = time.perf_counter()\n",
    "print(f\"Filtered in {toc - tic:0.4f} seconds\")\n",
    "print(month7_no_voucher.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53a95526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by column in 0.1234 seconds\n",
      "pyarrow.Table\n",
      "transaction_id: string\n",
      "store_id: int64\n",
      "payment_method_id: int64\n",
      "voucher_id: double\n",
      "user_id: double\n",
      "original_amount: double\n",
      "discount_applied: double\n",
      "final_amount: double\n",
      "created_at: string\n",
      "month: int32\n",
      "----\n",
      "transaction_id: [[\"2ae6d188-76c2-4095-b861-ab97d3cd9312\"]]\n",
      "store_id: [[4]]\n",
      "payment_method_id: [[5]]\n",
      "voucher_id: [[null]]\n",
      "user_id: [[null]]\n",
      "original_amount: [[38]]\n",
      "discount_applied: [[0]]\n",
      "final_amount: [[38]]\n",
      "created_at: [[\"2023-07-01 07:00:00\"]]\n",
      "month: [[202307]]\n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "filtered_table = table.filter(pa.compute.equal(table['month'], 202307))\n",
    "filtered_table = filtered_table.filter(pa.compute.equal(filtered_table['discount_applied'], 0))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Sorted by column in {toc - tic:0.4f} seconds\")\n",
    "top_row = filtered_table.slice(0, 1)\n",
    "print(top_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1e900",
   "metadata": {},
   "source": [
    "#### Grouping and Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c37ce9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated in 0.2226 seconds\n",
      "       final_amount                 \n",
      "                min   max <lambda_0>\n",
      "month                               \n",
      "202307         5.58  90.0  33.120199\n",
      "202308         1.00  90.0  32.933415\n",
      "202309         1.00  90.0  32.774340\n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "summary = df_from_pd.groupby([\"month\"]).agg({'final_amount': ['min', 'max', lambda x: np.mean(x)]})\n",
    "toc = time.perf_counter()\n",
    "print(f\"Aggregated in {toc - tic:0.4f} seconds\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcac16b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated in 0.0382 seconds\n",
      "pyarrow.Table\n",
      "month: int32\n",
      "final_amount_min: double\n",
      "final_amount_max: double\n",
      "final_amount_mean: double\n",
      "----\n",
      "month: [[202307,202308,202309]]\n",
      "final_amount_min: [[5.58,1,1]]\n",
      "final_amount_max: [[90,90,90]]\n",
      "final_amount_mean: [[33.12019873859749,32.93341456176001,32.77434009886071]]\n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "summary = table.group_by(\"month\").aggregate([\n",
    "   (\"final_amount\", \"min\"),\n",
    "   (\"final_amount\", \"max\"),\n",
    "   (\"final_amount\", \"mean\")\n",
    "])\n",
    "toc = time.perf_counter()\n",
    "print(f\"Aggregated in {toc - tic:0.4f} seconds\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f8fbe",
   "metadata": {},
   "source": [
    "As well as loading data in, performing common data tasks is much faster using PyArrow compared to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063983f6",
   "metadata": {},
   "source": [
    "#### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "371732ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged in 0.7031 seconds\n",
      "                         transaction_id  store_id  payment_method_id  \\\n",
      "0  2ae6d188-76c2-4095-b861-ab97d3cd9312         4                  5   \n",
      "1  7d0a474d-62f4-442a-96b6-a5df2bda8832         7                  1   \n",
      "2  85f86fef-fddb-4eef-9dc3-1444553e6108         1                  5   \n",
      "3  4c41d179-f809-4d5a-a5d7-acb25ae1fe98         5                  2   \n",
      "4  51e44c8e-4812-4a15-a9f9-9a46b62424d6         8                  5   \n",
      "\n",
      "   voucher_id  user_id  original_amount  discount_applied  final_amount  \\\n",
      "0         NaN      NaN             38.0               0.0          38.0   \n",
      "1         NaN      NaN             33.0               0.0          33.0   \n",
      "2         NaN      NaN             27.0               0.0          27.0   \n",
      "3         NaN      NaN             45.5               0.0          45.5   \n",
      "4         NaN      NaN             27.0               0.0          27.0   \n",
      "\n",
      "            created_at   month   location  \n",
      "0  2023-07-01 07:00:00  202307  Newcastle  \n",
      "1  2023-07-01 07:00:02  202307       Bath  \n",
      "2  2023-07-01 07:00:04  202307     London  \n",
      "3  2023-07-01 07:00:21  202307    Glasgow  \n",
      "4  2023-07-01 07:00:33  202307    Bristol  \n"
     ]
    }
   ],
   "source": [
    "# Create dataframe to merge\n",
    "locations = {\n",
    "\t'store_id': [\n",
    "\t\t1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "\t], \n",
    "\t'location': [\n",
    "\t\t'London',\n",
    "\t\t'Manchester',\n",
    "\t\t'Liverpool',\n",
    "\t\t'Newcastle',\n",
    "\t\t'Glasgow',\n",
    "\t\t'Norwich',\n",
    "\t\t'Bath',\n",
    "\t\t'Bristol',\n",
    "\t\t'Nottingham',\n",
    "\t\t'Coventry'\n",
    "\t]\n",
    "}\n",
    "stores_df = pd.DataFrame(locations)\n",
    "tic = time.perf_counter()\n",
    "merged_data = df_from_pd.merge(stores_df)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Merged in {toc - tic:0.4f} seconds\")\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03b35592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table as pandas DataFrame in 2.0802 seconds\n"
     ]
    }
   ],
   "source": [
    "# PyArrow does not have direct join functionality.\n",
    "# Time loading in and converting to pandas.\n",
    "tic = time.perf_counter()\n",
    "dataset = ds.dataset(\"datasets\", format=\"parquet\", partitioning=\"hive\")\n",
    "filter_months = ds.field(\"month\").isin([\"202307\", \"202308\", \"202309\"])\n",
    "table = dataset.to_table(filter=filter_months)\n",
    "df_from_arrow = table.to_pandas()\n",
    "toc = time.perf_counter()\n",
    "print(f\"Created table as pandas DataFrame in {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844d790",
   "metadata": {},
   "source": [
    "**PyArrow does not currently support join/merge functionality**. Instead, we could convert the table to a Pandas DataFrame. Loading the data from Parquet files and converting to a dataframe is **faster** than loading the data from CSV directly to a Pandas DataFrame.\n",
    "\n",
    "Alternatively, we can use **DuckDB**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04afaac",
   "metadata": {},
   "source": [
    "### DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71535d",
   "metadata": {},
   "source": [
    "[DuckDB](https://duckdb.org/) is a **\"fast open-source database system\"**. It has a plethora of capabilities; here, I'll be using an **in-memory** database, where no data is persisted to disk and all data is lost when a process finishes.\n",
    "\n",
    "DuckDB runs embedded inside our application (Python, R, Java, C++, etc.). It uses a columnar data layout and reads Parquet without loading the data into memory. There is **no server required**.\n",
    "\n",
    "Below are two examples: \n",
    "- Performing a join on two parquet files.\n",
    "- Performing aggregation on a csv file without loading the data into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f406086",
   "metadata": {},
   "source": [
    "#### Performing a Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c11291",
   "metadata": {},
   "source": [
    "The following code is used to save the `store_df` dataframe from earlier to Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1781a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations = {\n",
    "# \t'store_id': [\n",
    "# \t\t1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "# \t], \n",
    "# \t'location': [\n",
    "# \t\t'London',\n",
    "# \t\t'Manchester',\n",
    "# \t\t'Liverpool',\n",
    "# \t\t'Newcastle',\n",
    "# \t\t'Glasgow',\n",
    "# \t\t'Norwich',\n",
    "# \t\t'Bath',\n",
    "# \t\t'Bristol',\n",
    "# \t\t'Nottingham',\n",
    "# \t\t'Coventry'\n",
    "# \t]\n",
    "# }\n",
    "# store_df = pd.DataFrame(locations)\n",
    "# store_df.to_parquet(\"store_df.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6507ac",
   "metadata": {},
   "source": [
    "We connect to a local in-process DuckDB database (no server needed), then **execute SQL** on the desired parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9cc403d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB merge executed in 0.3978 seconds\n",
      "                              transaction_id  store_id  original_amount  \\\n",
      "0       2ae6d188-76c2-4095-b861-ab97d3cd9312         4             38.0   \n",
      "1       7d0a474d-62f4-442a-96b6-a5df2bda8832         7             33.0   \n",
      "2       4c41d179-f809-4d5a-a5d7-acb25ae1fe98         5             45.5   \n",
      "3       d449cf8f-e6d5-4b09-a02e-693c7889dee8         8             45.0   \n",
      "4       6b00c575-ec6e-4070-82d2-26d66b017b8b         3             77.0   \n",
      "...                                      ...       ...              ...   \n",
      "314897  e9915869-eabf-4fa8-9a48-a506f3313ecb         4             54.0   \n",
      "314898  40576233-9726-4fcc-bcb6-a623ad9f30e8         8             48.0   \n",
      "314899  ad2fe05e-7581-4d7b-8ec6-6d94ca29acfd         5             33.0   \n",
      "314900  9d6c44ee-e753-4527-a6ef-e2f1cc1c8c99         8             54.0   \n",
      "314901  161dfdb3-08df-411c-9221-12a7c2448066         2             64.0   \n",
      "\n",
      "          location  \n",
      "0        Newcastle  \n",
      "1             Bath  \n",
      "2          Glasgow  \n",
      "3          Bristol  \n",
      "4        Liverpool  \n",
      "...            ...  \n",
      "314897   Newcastle  \n",
      "314898     Bristol  \n",
      "314899     Glasgow  \n",
      "314900     Bristol  \n",
      "314901  Manchester  \n",
      "\n",
      "[314902 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect()\n",
    "transactions07 = \"datasets/month=202307\"\n",
    "locations = \"datasets/store_df.parquet\"\n",
    "tic = time.perf_counter()\n",
    "df = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        t1.transaction_id,\n",
    "        t1.store_id,\n",
    "\t\tt1.original_amount,\n",
    "\t\tt2.location\n",
    "    FROM read_parquet(?) AS t1\n",
    "    JOIN read_parquet(?) AS t2\n",
    "        USING (store_id)\n",
    "    WHERE t1.original_amount > 30\n",
    "\"\"\", [transactions07, locations]).df()\n",
    "toc = time.perf_counter()\n",
    "print(f\"DuckDB merge executed in {toc - tic:0.4f} seconds\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508f7b6",
   "metadata": {},
   "source": [
    "In one run, this took **0.2561** seconds!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c1a23",
   "metadata": {},
   "source": [
    "#### Aggregation with a CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafdc72a",
   "metadata": {},
   "source": [
    "We can also perform operations on data in a csv file without reading the data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12246b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB merge executed in 0.3518 seconds\n",
      "   store_id  n_transactions   avg_orig  avg_final\n",
      "0         1           62512  33.252536  33.091565\n",
      "1         2           61664  33.295999  33.140107\n",
      "2         3           61904  33.271566  33.113469\n",
      "3         4           61960  33.301404  33.142113\n",
      "4         5           61970  33.247475  33.092544\n",
      "5         6           62186  33.288513  33.136463\n",
      "6         7           62004  33.230695  33.074163\n",
      "7         8           62338  33.243110  33.080278\n",
      "8         9           61742  33.397468  33.236847\n",
      "9        10           61982  33.257252  33.095423\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect()\n",
    "tic = time.perf_counter()\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT\n",
    "\t\tstore_id,\n",
    "        COUNT(*) AS n_transactions,\n",
    "        AVG(original_amount) AS avg_orig,\n",
    "        AVG(final_amount) AS avg_final\n",
    "    FROM read_csv_auto('data/transactions/transactions_202307.csv')\n",
    "    GROUP BY store_id\n",
    "    ORDER BY store_id\n",
    "\"\"\").df()\n",
    "toc = time.perf_counter()\n",
    "print(f\"DuckDB merge executed in {toc - tic:0.4f} seconds\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d304a037",
   "metadata": {},
   "source": [
    "In one run this took **0.2613** seconds!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f116074",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e637383",
   "metadata": {},
   "source": [
    "It's safe to say that DuckDB is incredibly powerful, from its high speed operations to flexibility and low over-head. If you are already familiar with SQL, this is a nice, clean and fast approach.\n",
    "\n",
    "Despite the current limitation with performing joins in PyArrow, it is definitly worth using this when working with large amounts of data in memory. There is a clear speed performance benefit compared to Pandas, and for those familar with Pandas, there isn't a big difference in syntax.\n",
    "\n",
    "Finally, Parquet files are great way of storing lots of data. I found my local machine struggling to even store the csv example data in memory, but once I created the Parquet files this was not an issue. Partitioning the data is especially useful. This is a powerful data file format for storing and reading data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrow-bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
